{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b3135fe-f285-472e-a50a-8a13e9756cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17903323-1a2d-499b-a1c1-6f5e719099cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files2df_full(path):\n",
    "    all_files = glob.glob(path)\n",
    "    listu = []\n",
    "    for filename in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0, sep='\\t')\n",
    "            splitted_name = os.path.basename(filename).split('_')\n",
    "            name = splitted_name[2]\n",
    "            df['assay'] = name\n",
    "            listu.append(df)\n",
    "        except:\n",
    "            print(filename)\n",
    "    frame = pd.concat(listu, axis=0, ignore_index=True)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e28183b-6bdb-4ea7-8e18-9deb4e99e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_models_df = files2df_full('evaluated_models/*.tsv')\n",
    "evaluated_models_df.sampling = evaluated_models_df.sampling.fillna('original')\n",
    "evaluated_models_df['sampling'] = evaluated_models_df['sampling'].replace(['naive'], 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b52bc0ab-a1ea-4bfe-b416-891df1280123",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = []\n",
    "for row in evaluated_models_df['model']:\n",
    "    if row in ['ada', 'gbm', 'LogitBoost', 'xgbDART', 'xgbTree']:\n",
    "        algorithm.append('boosting')\n",
    "    elif row in ['AdaBag', 'treebag']:\n",
    "        algorithm.append('bagging')\n",
    "    elif row in ['knn', 'kknn']:\n",
    "        algorithm.append('kNN')\n",
    "    elif row in ['lda']:\n",
    "        algorithm.append('LDA')\n",
    "    elif row in ['bayes']:\n",
    "        algorithm.append('NB')\n",
    "    elif row in ['ranger', 'Rborist', 'rf']:\n",
    "        algorithm.append('RF')\n",
    "    elif row in ['C5.0']:\n",
    "        algorithm.append('C5.0')\n",
    "    elif row in ['regLogistic']:\n",
    "        algorithm.append('LR')\n",
    "    elif row in ['svmLinearWeights']:\n",
    "        algorithm.append('SVM')\n",
    "    elif row in ['nnet', 'pcaNNet']:\n",
    "        algorithm.append('ANN')\n",
    "    else:\n",
    "        algorithm.append('missing')\n",
    "        \n",
    "evaluated_models_df['algorithm'] = algorithm\n",
    "evaluated_models_df.to_csv('all_models_evaluated.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "105dd3b2-d8bb-494a-802f-463d9e402108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the models that have the lowest FPR at 90% of recall.\n",
    "selected = evaluated_models_df[evaluated_models_df.groupby(['assay'])['FPR_09'].rank('dense', ascending=True) == 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase_py",
   "language": "python",
   "name": "mybase_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
